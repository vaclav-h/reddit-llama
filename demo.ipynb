{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5d7e78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /mnt/appl/software/CUDA/11.4.1/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 114\n",
      "CUDA SETUP: Loading binary /home/halamvac/venvs/venv39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda114.so...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "from utils import prepare_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cce2016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AskRedditModel:\n",
    "    def __init__(self, model_path):\n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        model = LlamaForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
    "                                                 load_in_8bit=True,\n",
    "                                                 torch_dtype=torch.float16,\n",
    "                                                 device_map='auto')\n",
    "        self.model = PeftModel.from_pretrained(model, model_path)\n",
    "        self.tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "            \n",
    "    def __call__(self, question, min_length=20):\n",
    "        prompt = prepare_prompt(question)\n",
    "        inp = self.tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "        generated = self.model.generate(input_ids=inp.to(self.device),\n",
    "                                        no_repeat_ngram_size=3,\n",
    "                                        num_beams=4,\n",
    "                                        max_new_tokens=128,\n",
    "                                        min_new_tokens=min_length,\n",
    "                                        early_stopping=True)\n",
    "        response = self.tokenizer.decode(generated[0])\n",
    "        \n",
    "        # Clean the output\n",
    "        return response.split(\"Response:\")[1][:-4]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94911b74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae0decd545c4ca8945ffcbd62a35f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AskRedditModel(\"askreddit_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbf6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/halamvac/venvs/venv39/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mosquitoes. They've killed more people than any other animal in the history of the planet.\n"
     ]
    }
   ],
   "source": [
    "resp = model(\"What is something that is way more dangerous than people think it is?\", min_length=20)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "794cfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inside of a toilet.\n",
      "\n",
      "You can clean it, but it'll still be dirty.\n"
     ]
    }
   ],
   "source": [
    "resp = model(\"What will always be dirty no matter how often it's cleaned?\", min_length=20)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72fe9e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics\n"
     ]
    }
   ],
   "source": [
    "resp = model(\"What do people take way too seriously?\", min_length=0)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c78348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mental health\n"
     ]
    }
   ],
   "source": [
    "resp = model(\"What is something that people don't take seriously enough?\", min_length=0)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e72b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japanese toilet paper is the best\n"
     ]
    }
   ],
   "source": [
    "resp = model(\"What does japan do better than the rest of the world?\", min_length=10)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79afa30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
